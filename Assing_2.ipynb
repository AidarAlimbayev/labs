{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment 2 Advanced Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the Assignment\n",
    "You need to work with three image datasets that contain class-conditional random label noise:\n",
    "\n",
    "FashionMNIST0.3.npz - Known transition matrix\n",
    "FashionMNIST0.6.npz - Known transition matrix\n",
    "CIFAR.npz - Unknown transition matrix (requires estimation)\n",
    "\n",
    "Your main tasks are:\n",
    "\n",
    "Implement at least two classification algorithms robust to label noise\n",
    "Build a transition matrix estimator for the third dataset\n",
    "Compare the robustness of your algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from seaborn) (2.1.3)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 seaborn-0.13.2 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = np.load('/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/CIFAR10.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile '/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/CIFAR10.npz' with keys: X_tr, S_tr, X_ts, Y_ts"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X_train = dataset['X_tr']  # Training features\n",
    "S_train = dataset['S_tr']  # Noisy labels for training\n",
    "X_test = dataset['X_ts']  # Test features\n",
    "Y_test = dataset['Y_ts']  # Clean labels for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, S_train, S_val = train_test_split(X_train, S_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing (normalize, reshape if needed)\n",
    "# For images, you might want to normalize the pixel values to [0,1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# # Reshape if needed\n",
    "# X_train = X_train.reshape(-1, 32, 32, 3)\n",
    "# X_val = X_val.reshape(-1, 32, 32, 3)\n",
    "# X_ts = X_ts.reshape(-1, 32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_f = np.load('/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/FashionMNIST0.3.npz')\n",
    "X_train = dataset_f['X_tr']  # Training features\n",
    "S_train = dataset_f['S_tr']  # Noisy labels for training\n",
    "X_test = dataset_f['X_ts']  # Test features\n",
    "Y_test = dataset_f['Y_ts']  # Clean labels for testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #X_train\n",
    "# X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_matrix(features, noisy_labels, num_classes=4):\n",
    "    # Train a base classifier (e.g., neural network, random forest)\n",
    "    model = train_base_classifier(features, noisy_labels)\n",
    "    \n",
    "    # Get predicted probabilities on training data\n",
    "    pred_probs = model.predict_proba(features)\n",
    "    \n",
    "    # Estimate transition matrix using predicted probabilities\n",
    "    T_estimated = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # Implementation details here...\n",
    "    \n",
    "    return T_estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some example of CIFAR CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cifar_cnn():\n",
    "    \"\"\"\n",
    "    Creates a deeper CNN suitable for CIFAR classification.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(32, 32, 3)),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4)  # 4 classes\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_correction_loss(y_true, y_pred, T):\n",
    "    \"\"\"\n",
    "    Forward correction loss using transition matrix T\n",
    "    \"\"\"\n",
    "    # Convert one-hot to sparse if needed\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    pred_probs = tf.nn.softmax(y_pred)\n",
    "    \n",
    "    # Apply transition matrix correction\n",
    "    corrected_probs = tf.matmul(pred_probs, tf.constant(T, dtype=tf.float32))\n",
    "    \n",
    "    # Calculate cross-entropy loss with corrected probabilities\n",
    "    loss = tf.nn.sparse_categorical_crossentropy(y_true, corrected_probs)\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Create and compile model with custom loss\n",
    "model = create_cifar_cnn()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: forward_correction_loss(y_true, y_pred, T),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs=3):\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # Random seed for reproducibility but different splits\n",
    "        seed = 42 + i\n",
    "        \n",
    "        # Split data into train/validation\n",
    "        X_train, X_val, S_train, S_val = train_test_split(\n",
    "            Xtr, Str, test_size=0.2, random_state=seed\n",
    "        )\n",
    "        \n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, S_train, X_val, S_val)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        accuracy = classifier.evaluate(Xts, Yts)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    return mean_acc, std_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NoiseRobustClassifier:\n",
    "    def __init__(self, name, model_builder, transition_matrix=None, use_correction=True):\n",
    "        \"\"\"\n",
    "        A wrapper for noise-robust classifiers.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the classifier for reporting\n",
    "            model_builder: Function that returns a TF/Keras model\n",
    "            transition_matrix: The noise transition matrix if known\n",
    "            use_correction: Whether to use transition matrix correction\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model_builder = model_builder\n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.use_correction = use_correction\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def forward_correction_loss(self, y_true, y_pred):\n",
    "        \"\"\"Forward correction loss using transition matrix\"\"\"\n",
    "        # Handle case when y_true is one-hot encoded\n",
    "        if len(tf.shape(y_true)) > 1:\n",
    "            y_true = tf.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Get predicted probabilities\n",
    "        pred_probs = tf.nn.softmax(y_pred)\n",
    "        \n",
    "        # Apply transition matrix correction\n",
    "        T = tf.constant(self.transition_matrix, dtype=tf.float32)\n",
    "        corrected_probs = tf.matmul(pred_probs, T)\n",
    "        \n",
    "        # Calculate cross-entropy loss with corrected probabilities\n",
    "        loss = tf.nn.sparse_categorical_crossentropy(y_true, corrected_probs)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def fit(self, X_train, S_train, X_val, S_val, epochs=30, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the classifier on noisy data and validate.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            S_train: Noisy training labels\n",
    "            X_val: Validation features\n",
    "            S_val: Noisy validation labels\n",
    "        \"\"\"\n",
    "        # Build a new model (to reset weights for multiple runs)\n",
    "        self.model = self.model_builder()\n",
    "        \n",
    "        # Configure the loss function based on whether we use correction\n",
    "        if self.transition_matrix is not None and self.use_correction:\n",
    "            loss = self.forward_correction_loss\n",
    "        else:\n",
    "            # If no transition matrix provided or correction disabled, use standard CE\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "        \n",
    "        # Compile the model\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=loss,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, S_train,\n",
    "            validation_data=(X_val, S_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,  # Silence output for multiple runs\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on clean test data.\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "            Y_test: Clean test labels\n",
    "            \n",
    "        Returns:\n",
    "            Accuracy on the test set\n",
    "        \"\"\"\n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred_classes == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(datasets, classifiers, n_runs=3):\n",
    "    \"\"\"\n",
    "    Run evaluations for multiple classifiers on multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dict of {dataset_name: (Xtr, Str, Xts, Yts)}\n",
    "        classifiers: List of NoiseRobustClassifier instances\n",
    "        n_runs: Number of runs with different random splits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results {dataset_name: {classifier_name: (mean_acc, std_acc)}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, (Xtr, Str, Xts, Yts) in datasets.items():\n",
    "        print(f\"Evaluating on dataset: {dataset_name}\")\n",
    "        dataset_results = {}\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            print(f\"  - Evaluating classifier: {classifier.name}\")\n",
    "            mean_acc, std_acc = evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs)\n",
    "            dataset_results[classifier.name] = (mean_acc, std_acc)\n",
    "            print(f\"    Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        \n",
    "        results[dataset_name] = dataset_results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results, title=\"Classifier Performance Comparison\"):\n",
    "    \"\"\"\n",
    "    Create bar plots to visualize classifier performance across datasets.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Determine dimensions\n",
    "    n_datasets = len(results)\n",
    "    n_classifiers = len(next(iter(results.values())))\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 5 * n_datasets))\n",
    "    \n",
    "    # For each dataset\n",
    "    for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "        # Create subplot\n",
    "        plt.subplot(n_datasets, 1, i+1)\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        classifiers = list(dataset_results.keys())\n",
    "        accuracies = [dataset_results[clf][0] for clf in classifiers]  # mean\n",
    "        errors = [dataset_results[clf][1] for clf in classifiers]      # std\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(classifiers, accuracies, yerr=errors, capsize=10)\n",
    "        \n",
    "        # Add labels and grid\n",
    "        plt.title(f\"{dataset_name} - Test Accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                     bar.get_height() + 0.01, \n",
    "                     f\"{acc:.4f}\", \n",
    "                     ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(classifier, datasets, title=\"Confusion Matrices\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for a classifier on all datasets.\n",
    "    \n",
    "    Args:\n",
    "        classifier: A trained NoiseRobustClassifier instance\n",
    "        datasets: Dict of {dataset_name: (Xtr, Str, Xts, Yts)}\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    # Set up the figure\n",
    "    n_datasets = len(datasets)\n",
    "    fig, axes = plt.subplots(1, n_datasets, figsize=(5 * n_datasets, 5))\n",
    "    \n",
    "    # If only one dataset, make axes iterable\n",
    "    if n_datasets == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # For each dataset\n",
    "    for i, (dataset_name, (Xtr, Str, Xts, Yts)) in enumerate(datasets.items()):\n",
    "        # Get predictions\n",
    "        y_pred = classifier.model.predict(Xts, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(Yts, y_pred_classes)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "        axes[i].set_title(f\"{dataset_name} - {classifier.name}\")\n",
    "        axes[i].set_xlabel(\"Predicted Label\")\n",
    "        axes[i].set_ylabel(\"True Label\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Evaluating on dataset: FashionMNIST_0.3\n",
      "  - Evaluating classifier: Standard CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.eye(num_classes)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    108\u001b[39m classifiers = [\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# Standard CNN without correction\u001b[39;00m\n\u001b[32m    110\u001b[39m     NoiseRobustClassifier(\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     )\n\u001b[32m    130\u001b[39m ]\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Run evaluations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m results = \u001b[43mrun_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[32m    136\u001b[39m visualize_results(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_evaluations\u001b[39m\u001b[34m(datasets, classifiers, n_runs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Evaluating classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     mean_acc, std_acc = \u001b[43mevaluate_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     dataset_results[classifier.name] = (mean_acc, std_acc)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mevaluate_classifier\u001b[39m\u001b[34m(classifier, Xtr, Str, Xts, Yts, n_runs)\u001b[39m\n\u001b[32m     12\u001b[39m X_train, X_val, S_train, S_val = train_test_split(\n\u001b[32m     13\u001b[39m     Xtr, Str, test_size=\u001b[32m0.2\u001b[39m, random_state=seed\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     20\u001b[39m accuracy = classifier.evaluate(Xts, Yts)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mNoiseRobustClassifier.fit\u001b[39m\u001b[34m(self, X_train, S_train, X_val, S_val, epochs, batch_size)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.model.compile(\n\u001b[32m     59\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     60\u001b[39m     loss=loss,\n\u001b[32m     61\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     62\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Silence output for multiple runs\u001b[39;49;00m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/assing_2/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the evaluation framework.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # FashionMNIST 0.3 noise\n",
    "    dataset1 = np.load('/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/FashionMNIST0.3.npz')\n",
    "    # Load dataset\n",
    "    X_train = dataset1['X_tr']  # Training features\n",
    "    S_train = dataset1['S_tr']  # Noisy labels for training\n",
    "    X_test = dataset1['X_ts']  # Test features\n",
    "    Y_test = dataset1['Y_ts']  # Clean labels for testing\n",
    "    \n",
    "    Xtr1, Str1 = dataset1['X_tr'], dataset1['S_tr']\n",
    "    Xts1, Yts1 = dataset1['X_ts'], dataset1['Y_ts']\n",
    "    \n",
    "    # Preprocess data (normalize pixel values)\n",
    "    Xtr1 = Xtr1.astype('float32') / 255.0\n",
    "    Xts1 = Xts1.astype('float32') / 255.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # FashionMNIST 0.6 noise\n",
    "    dataset2 = np.load('/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/FashionMNIST0.6.npz')\n",
    "    Xtr2, Str2 = dataset2['X_tr'], dataset2['S_tr']\n",
    "    Xts2, Yts2 = dataset2['X_ts'], dataset2['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr2 = Xtr2.astype('float32') / 255.0\n",
    "    Xts2 = Xts2.astype('float32') / 255.0\n",
    "    \n",
    "    # CIFAR - unknown transition matrix\n",
    "    dataset3 = np.load('/Users/Aidar Alimbayev/Documents/projects_aidar_mac/ml805_labs/Assingment 2 /datasets/CIFAR10.npz')\n",
    "    Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "    Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "    Xts3 = Xts3.astype('float32') / 255.0\n",
    "    \n",
    "    # Define transition matrices\n",
    "    T_fashion_03 = np.array([\n",
    "        [0.7, 0.3, 0, 0],\n",
    "        [0, 0.7, 0.3, 0],\n",
    "        [0, 0, 0.7, 0.3],\n",
    "        [0.3, 0, 0, 0.7]\n",
    "    ])\n",
    "    \n",
    "    T_fashion_06 = np.array([\n",
    "        [0.4, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.4, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.4, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.4]\n",
    "    ])\n",
    "    \n",
    "    # Create transition matrix estimator for CIFAR\n",
    "    # This is a placeholder - you'll implement your own estimator\n",
    "    T_cifar_estimated = estimate_transition_matrix(Xtr3, Str3)\n",
    "    \n",
    "    # Organize datasets\n",
    "    datasets = {\n",
    "        'FashionMNIST_0.3': (Xtr1, Str1, Xts1, Yts1),\n",
    "        'FashionMNIST_0.6': (Xtr2, Str2, Xts2, Yts2),\n",
    "        'CIFAR': (Xtr3, Str3, Xts3, Yts3)\n",
    "    }\n",
    "    \n",
    "    # Define model builders\n",
    "    def build_fashion_mnist_cnn():\n",
    "        model = models.Sequential([\n",
    "            layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def build_cifar_cnn():\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(32, 32, 3)),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Create classifiers\n",
    "    classifiers = [\n",
    "        # Standard CNN without correction\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Standard CNN\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            use_correction=False\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_03,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Mean Absolute Error loss (inherently robust to noise)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"MAE Loss\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            use_correction=False\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = run_evaluations(datasets, classifiers, n_runs=3)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Plot confusion matrices for the best classifier\n",
    "    best_classifier = classifiers[1]  # Assuming Forward Correction is best\n",
    "    plot_confusion_matrices(best_classifier, datasets)\n",
    "\n",
    "# Placeholder for transition matrix estimator (you will implement this)\n",
    "def estimate_transition_matrix(X, S, num_classes=4):\n",
    "    \"\"\"\n",
    "    Estimate the transition matrix from noisy data.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        S: Noisy labels\n",
    "        num_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        Estimated transition matrix\n",
    "    \"\"\"\n",
    "    # This is a placeholder - you will implement your own estimation method\n",
    "    # For now, just return an identity matrix (no correction)\n",
    "    return np.eye(num_classes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_matrix_anchor_points(X, S, num_classes=4, anchor_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Estimate transition matrix using anchor points method.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        S: Noisy labels\n",
    "        num_classes: Number of classes\n",
    "        anchor_threshold: Confidence threshold for anchor points\n",
    "        \n",
    "    Returns:\n",
    "        Estimated transition matrix\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data for initial model training\n",
    "    X_train, X_val, S_train, S_val = train_test_split(X, S, test_size=0.2)\n",
    "    \n",
    "    # Create and train a base model\n",
    "    base_model = models.Sequential([\n",
    "        layers.Input(shape=X.shape[1:]),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    base_model.fit(X_train, S_train, validation_data=(X_val, S_val), \n",
    "                   epochs=10, verbose=0)\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    pred_probs = base_model.predict(X)\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # For each true class, find anchor points and estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        # Find anchor points: points where the model is very confident about class i\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            # Count occurrences of each noisy label among anchor points\n",
    "            for j in range(num_classes):\n",
    "                class_j_count = np.sum(S[anchor_idx] == j)\n",
    "                T[i, j] = class_j_count / len(anchor_idx)\n",
    "        else:\n",
    "            # If no anchor points found, use a fallback (identity or prior knowledge)\n",
    "            T[i, i] = 0.7  # Assuming diagonal dominance as a prior\n",
    "            T[i, (i+1) % num_classes] = 0.3  # Some off-diagonal probability\n",
    "    \n",
    "    # Ensure each row sums to 1\n",
    "    row_sums = T.sum(axis=1, keepdims=True)\n",
    "    T = T / row_sums\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/assing_2/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-macosx_10_13_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transition matrix A\n",
    "A = np.array([[0.2987, 0],\n",
    "              [0, 0.134]])\n",
    "\n",
    "B = np.array([[0.5, 0.71],\n",
    "              [0.352, 0.5],\n",
    "              [0.352, 0.5],\n",
    "              [0.5, 0.71]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2987, 0.    ],\n",
       "       [0.    , 0.134 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.71 ],\n",
       "       [0.352, 0.5  ],\n",
       "       [0.352, 0.5  ],\n",
       "       [0.5  , 0.71 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[0.5, 0.352],\n",
    "              [0.71, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09127814 0.1296225 ]\n",
      " [0.1296225  0.18407467]]\n"
     ]
    }
   ],
   "source": [
    "C =  M @ A @ M.T\n",
    "\n",
    "print(C)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14935  , 0.1051424],\n",
       "       [0.09514  , 0.067    ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.matmul(A, M)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.array([[0.352, 0.5],\n",
    "              [0.5, 0.71]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07051012, 0.1001412 ],\n",
       "       [0.1001412 , 0.1422244 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = N @ A @ N.T\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
