{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:27:10.696332: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-14 23:27:10.716372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-14 23:27:10.733565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-14 23:27:10.738847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 23:27:10.751676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-14 23:27:15.787803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU instead\n",
      "TensorFlow is using devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "TensorFlow CUDA available: True\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3714183/3964049681.py:28: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "TensorFlow GPU available: False\n",
      "Starting evaluation framework on CUDA:5\n",
      "======================================\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:27:26.134723: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-03-14 23:27:26.145112: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating transition matrix for CIFAR dataset...\n",
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.3816 - val_loss: 1.2084\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.2410 - val_loss: 1.1491\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1978 - val_loss: 1.1679\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1.1900 - val_loss: 1.1504\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1612 - val_loss: 1.1626\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1722 - val_loss: 1.1485\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1527 - val_loss: 1.1345\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1431 - val_loss: 1.1375\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1373 - val_loss: 1.0967\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1.1245 - val_loss: 1.1224\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Estimated transition matrix for CIFAR:\n",
      "[[0.7        0.3        0.         0.        ]\n",
      " [0.03448276 0.89655172 0.06896552 0.        ]\n",
      " [0.25       0.         0.         0.75      ]\n",
      " [0.3        0.         0.         0.7       ]]\n",
      "\n",
      "=== Evaluating on dataset: FashionMNIST_0.3 ===\n",
      "\n",
      "--- Evaluating classifier: Standard CNN ---\n",
      "Run 1/3 - Training Standard CNN...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidar.alimbayev/anaconda3/envs/ml817/lib/python3.10/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.2760 - loss: 1.8664 - val_accuracy: 0.2433 - val_loss: 4.9364\n",
      "Epoch 2/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2868 - loss: 1.3984 - val_accuracy: 0.3512 - val_loss: 2.1334\n",
      "Epoch 3/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2921 - loss: 1.3914 - val_accuracy: 0.2406 - val_loss: 1.3948\n",
      "Epoch 4/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2650 - loss: 1.4020 - val_accuracy: 0.3392 - val_loss: 1.3863\n",
      "Epoch 5/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2977 - loss: 1.3909 - val_accuracy: 0.3388 - val_loss: 1.3863\n",
      "Epoch 6/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.3000 - loss: 1.3895 - val_accuracy: 0.3385 - val_loss: 1.3863\n",
      "Epoch 7/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.3058 - loss: 1.3863 - val_accuracy: 0.3385 - val_loss: 1.3863\n",
      "Epoch 8/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2987 - loss: 1.3867 - val_accuracy: 0.3385 - val_loss: 1.3863\n",
      "Epoch 9/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2990 - loss: 1.3870 - val_accuracy: 0.3385 - val_loss: 1.3863\n",
      "Run 1/3 - Test accuracy: 0.4525\n",
      "Run 2/3 - Training Standard CNN...\n",
      "Epoch 1/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.2882 - loss: 3.2193 - val_accuracy: 0.2115 - val_loss: 8.2148\n",
      "Epoch 2/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2477 - loss: 1.4074 - val_accuracy: 0.3319 - val_loss: 2.9655\n",
      "Epoch 3/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2441 - loss: 1.3974 - val_accuracy: 0.2840 - val_loss: 1.4203\n",
      "Epoch 4/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2607 - loss: 1.4049 - val_accuracy: 0.2448 - val_loss: 1.3895\n",
      "Epoch 5/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2436 - loss: 1.3941 - val_accuracy: 0.2237 - val_loss: 1.3863\n",
      "Epoch 6/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2545 - loss: 1.3902 - val_accuracy: 0.2650 - val_loss: 1.3863\n",
      "Epoch 7/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2467 - loss: 1.3948 - val_accuracy: 0.3040 - val_loss: 1.3860\n",
      "Epoch 8/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2466 - loss: 1.3949 - val_accuracy: 0.3092 - val_loss: 1.3863\n",
      "Epoch 9/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2472 - loss: 1.3869 - val_accuracy: 0.3046 - val_loss: 1.3863\n",
      "Epoch 10/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2387 - loss: 1.3865 - val_accuracy: 0.3123 - val_loss: 1.3863\n",
      "Epoch 11/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2483 - loss: 1.3905 - val_accuracy: 0.3056 - val_loss: 1.3863\n",
      "Epoch 12/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2443 - loss: 1.3869 - val_accuracy: 0.3060 - val_loss: 1.3863\n",
      "Run 2/3 - Test accuracy: 0.3392\n",
      "Run 3/3 - Training Standard CNN...\n",
      "Epoch 1/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.2827 - loss: 3.7053 - val_accuracy: 0.2508 - val_loss: 6.2630\n",
      "Epoch 2/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2814 - loss: 1.3930 - val_accuracy: 0.2765 - val_loss: 2.3253\n",
      "Epoch 3/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.2811 - loss: 1.3911 - val_accuracy: 0.2473 - val_loss: 1.4106\n",
      "Epoch 4/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2782 - loss: 1.3879 - val_accuracy: 0.2537 - val_loss: 1.3863\n",
      "Epoch 5/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2776 - loss: 1.3865 - val_accuracy: 0.2548 - val_loss: 1.3863\n",
      "Epoch 6/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.2779 - loss: 1.3882 - val_accuracy: 0.2548 - val_loss: 1.3863\n",
      "Epoch 7/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.2892 - loss: 1.3901 - val_accuracy: 0.2910 - val_loss: 1.3863\n",
      "Epoch 8/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2831 - loss: 1.3895 - val_accuracy: 0.2556 - val_loss: 1.3863\n",
      "Epoch 9/30\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2869 - loss: 1.3960 - val_accuracy: 0.3090 - val_loss: 1.3863\n",
      "Run 3/3 - Test accuracy: 0.2517\n",
      "Final results - Accuracy: 0.3478 ± 0.0822\n",
      "\n",
      "--- Evaluating classifier: Forward Correction (T_0.3) ---\n",
      "Run 1/3 - Training Forward Correction (T_0.3)...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len is not well defined for a symbolic Tensor (compile_loss/forward_correction_loss/Shape:0). Please call `x.shape` rather than `len(x)` for shape information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 491\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 474\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    440\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# Standard CNN without correction\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     NoiseRobustClassifier(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m     )\n\u001b[1;32m    471\u001b[0m ]\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# Run evaluations\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    477\u001b[0m visualize_results(results)\n",
      "Cell \u001b[0;32mIn[1], line 196\u001b[0m, in \u001b[0;36mrun_evaluations\u001b[0;34m(datasets, classifiers, n_runs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Evaluating classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m     mean_acc, std_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     dataset_results[classifier\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m (mean_acc, std_acc)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal results - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 163\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(classifier, Xtr, Str, Xts, Yts, n_runs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m    166\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mevaluate(Xts, Yts)\n",
      "Cell \u001b[0;32mIn[1], line 96\u001b[0m, in \u001b[0;36mNoiseRobustClassifier.fit\u001b[0;34m(self, X_train, S_train, X_val, S_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     90\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     91\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     92\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Show progress for monitoring\u001b[39;49;00m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml817/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36mNoiseRobustClassifier.forward_correction_loss\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward correction loss using transition matrix\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Handle case when y_true is one-hot encoded\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     52\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(y_true, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Get predicted probabilities\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: len is not well defined for a symbolic Tensor (compile_loss/forward_correction_loss/Shape:0). Please call `x.shape` rather than `len(x)` for shape information."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set CUDA device before importing TensorFlow\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"  # Make only GPU 5 visible to TensorFlow\n",
    "\n",
    "# Configure TensorFlow to use GPU memory growth to avoid taking all GPU memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Configure TensorFlow to use memory growth\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(f\"Using GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead\")\n",
    "\n",
    "# Verify CUDA configuration\n",
    "print(f\"TensorFlow is using devices: {tf.config.list_physical_devices()}\")\n",
    "print(f\"TensorFlow CUDA available: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"TensorFlow GPU available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "class NoiseRobustClassifier:\n",
    "    def __init__(self, name, model_builder, transition_matrix=None, use_correction=True):\n",
    "        \"\"\"\n",
    "        A wrapper for noise-robust classifiers.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the classifier for reporting\n",
    "            model_builder: Function that returns a TF/Keras model\n",
    "            transition_matrix: The noise transition matrix if known\n",
    "            use_correction: Whether to use transition matrix correction\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model_builder = model_builder\n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.use_correction = use_correction\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def forward_correction_loss(self, y_true, y_pred):\n",
    "        \"\"\"Forward correction loss using transition matrix\"\"\"\n",
    "        # Handle case when y_true is one-hot encoded\n",
    "        if len(tf.shape(y_true)) > 1:\n",
    "            y_true = tf.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Get predicted probabilities\n",
    "        pred_probs = tf.nn.softmax(y_pred)\n",
    "        \n",
    "        # Apply transition matrix correction\n",
    "        T = tf.constant(self.transition_matrix, dtype=tf.float32)\n",
    "        corrected_probs = tf.matmul(pred_probs, T)\n",
    "        \n",
    "        # Calculate cross-entropy loss with corrected probabilities\n",
    "        loss = tf.nn.sparse_categorical_crossentropy(y_true, corrected_probs)\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def fit(self, X_train, S_train, X_val, S_val, epochs=30, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the classifier on noisy data and validate.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            S_train: Noisy training labels\n",
    "            X_val: Validation features\n",
    "            S_val: Noisy validation labels\n",
    "        \"\"\"\n",
    "        # Use explicit device placement to ensure operations run on the specified GPU\n",
    "        with tf.device('/device:GPU:0'):  # This refers to the first (and only) visible GPU in the environment\n",
    "            # Build a new model (to reset weights for multiple runs)\n",
    "            self.model = self.model_builder()\n",
    "            \n",
    "            # Configure the loss function based on whether we use correction\n",
    "            if self.transition_matrix is not None and self.use_correction:\n",
    "                loss = self.forward_correction_loss\n",
    "            else:\n",
    "                # If no transition matrix provided or correction disabled, use standard CE\n",
    "                loss = 'sparse_categorical_crossentropy'\n",
    "            \n",
    "            # Compile the model\n",
    "            self.model.compile(\n",
    "                optimizer='adam',\n",
    "                loss=loss,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            self.history = self.model.fit(\n",
    "                X_train, S_train,\n",
    "                validation_data=(X_val, S_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                verbose=1,  # Show progress for monitoring\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=5,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the classifier on clean test data.\n",
    "        \n",
    "        Args:\n",
    "            X_test: Test features\n",
    "            Y_test: Clean test labels\n",
    "            \n",
    "        Returns:\n",
    "            Accuracy on the test set\n",
    "        \"\"\"\n",
    "        # Make predictions\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            y_pred = self.model.predict(X_test, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred_classes == Y_test)\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "def evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs=3):\n",
    "    \"\"\"\n",
    "    Evaluate a classifier multiple times with different random splits.\n",
    "    \n",
    "    Args:\n",
    "        classifier: A NoiseRobustClassifier instance\n",
    "        Xtr: Training features\n",
    "        Str: Noisy training labels\n",
    "        Xts: Test features\n",
    "        Yts: Clean test labels\n",
    "        n_runs: Number of evaluation runs\n",
    "        \n",
    "    Returns:\n",
    "        mean_acc, std_acc: Mean and standard deviation of test accuracies\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # Random seed for reproducibility but different splits\n",
    "        seed = 42 + i\n",
    "        \n",
    "        # Split data into train/validation\n",
    "        X_train, X_val, S_train, S_val = train_test_split(\n",
    "            Xtr, Str, test_size=0.2, random_state=seed\n",
    "        )\n",
    "        \n",
    "        print(f\"Run {i+1}/{n_runs} - Training {classifier.name}...\")\n",
    "        \n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, S_train, X_val, S_val)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        accuracy = classifier.evaluate(Xts, Yts)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Run {i+1}/{n_runs} - Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    return mean_acc, std_acc\n",
    "\n",
    "def run_evaluations(datasets, classifiers, n_runs=3):\n",
    "    \"\"\"\n",
    "    Run evaluations for multiple classifiers on multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dict of {dataset_name: (Xtr, Str, Xts, Yts)}\n",
    "        classifiers: List of NoiseRobustClassifier instances\n",
    "        n_runs: Number of runs with different random splits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results {dataset_name: {classifier_name: (mean_acc, std_acc)}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, (Xtr, Str, Xts, Yts) in datasets.items():\n",
    "        print(f\"\\n=== Evaluating on dataset: {dataset_name} ===\")\n",
    "        dataset_results = {}\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            print(f\"\\n--- Evaluating classifier: {classifier.name} ---\")\n",
    "            mean_acc, std_acc = evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs)\n",
    "            dataset_results[classifier.name] = (mean_acc, std_acc)\n",
    "            print(f\"Final results - Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        \n",
    "        results[dataset_name] = dataset_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results, title=\"Classifier Performance Comparison\"):\n",
    "    \"\"\"\n",
    "    Create bar plots to visualize classifier performance across datasets.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Determine dimensions\n",
    "    n_datasets = len(results)\n",
    "    n_classifiers = len(next(iter(results.values())))\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 5 * n_datasets))\n",
    "    \n",
    "    # For each dataset\n",
    "    for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "        # Create subplot\n",
    "        plt.subplot(n_datasets, 1, i+1)\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        classifiers = list(dataset_results.keys())\n",
    "        accuracies = [dataset_results[clf][0] for clf in classifiers]  # mean\n",
    "        errors = [dataset_results[clf][1] for clf in classifiers]      # std\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(classifiers, accuracies, yerr=errors, capsize=10)\n",
    "        \n",
    "        # Add labels and grid\n",
    "        plt.title(f\"{dataset_name} - Test Accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                     bar.get_height() + 0.01, \n",
    "                     f\"{acc:.4f}\", \n",
    "                     ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.savefig(\"classifier_performance.png\")\n",
    "    plt.show()\n",
    "\n",
    "def estimate_transition_matrix_anchor_points(X, S, num_classes=4, anchor_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Estimate transition matrix using anchor points method.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        S: Noisy labels\n",
    "        num_classes: Number of classes\n",
    "        anchor_threshold: Confidence threshold for anchor points\n",
    "        \n",
    "    Returns:\n",
    "        Estimated transition matrix\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data for initial model training\n",
    "    X_train, X_val, S_train, S_val = train_test_split(X, S, test_size=0.2)\n",
    "    \n",
    "    # Create and train a base model\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        base_model = models.Sequential([\n",
    "            layers.Input(shape=X.shape[1:]),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "        base_model.fit(X_train, S_train, validation_data=(X_val, S_val), \n",
    "                       epochs=10, verbose=1)\n",
    "        \n",
    "        # Get predicted probabilities\n",
    "        pred_probs = base_model.predict(X)\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # For each true class, find anchor points and estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        # Find anchor points: points where the model is very confident about class i\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            # Count occurrences of each noisy label among anchor points\n",
    "            for j in range(num_classes):\n",
    "                class_j_count = np.sum(S[anchor_idx] == j)\n",
    "                T[i, j] = class_j_count / len(anchor_idx)\n",
    "        else:\n",
    "            # If no anchor points found, use a fallback (identity or prior knowledge)\n",
    "            T[i, i] = 0.7  # Assuming diagonal dominance as a prior\n",
    "            T[i, (i+1) % num_classes] = 0.3  # Some off-diagonal probability\n",
    "    \n",
    "    # Ensure each row sums to 1\n",
    "    row_sums = T.sum(axis=1, keepdims=True)\n",
    "    T = T / row_sums\n",
    "    \n",
    "    return T\n",
    "\n",
    "def generate_results_table(results):\n",
    "    \"\"\"\n",
    "    Generate a formatted results table for the report.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with results table\n",
    "    \"\"\"\n",
    "    # Create header\n",
    "    table = \"| Dataset | Classifier | Accuracy (%) | Std. Dev. |\\n\"\n",
    "    table += \"|---------|------------|-------------|----------|\\n\"\n",
    "    \n",
    "    # Add rows\n",
    "    for dataset_name, dataset_results in results.items():\n",
    "        for i, (classifier_name, (mean_acc, std_acc)) in enumerate(dataset_results.items()):\n",
    "            # First row of a dataset gets the dataset name\n",
    "            if i == 0:\n",
    "                table += f\"| {dataset_name} | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "            else:\n",
    "                table += f\"| | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "    \n",
    "    return table\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the evaluation framework on GPU.\n",
    "    \"\"\"\n",
    "    print(\"Starting evaluation framework on CUDA:5\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # FashionMNIST 0.3 noise\n",
    "    dataset1 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.3.npz')\n",
    "    Xtr1, Str1 = dataset1['X_tr'], dataset1['S_tr']\n",
    "    Xts1, Yts1 = dataset1['X_ts'], dataset1['Y_ts']\n",
    "    \n",
    "    # Preprocess data (normalize pixel values)\n",
    "    Xtr1 = Xtr1.astype('float32') / 255.0\n",
    "    Xts1 = Xts1.astype('float32') / 255.0\n",
    "    \n",
    "    # FashionMNIST 0.6 noise\n",
    "    dataset2 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.6.npz')\n",
    "    Xtr2, Str2 = dataset2['X_tr'], dataset2['S_tr']\n",
    "    Xts2, Yts2 = dataset2['X_ts'], dataset2['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr2 = Xtr2.astype('float32') / 255.0\n",
    "    Xts2 = Xts2.astype('float32') / 255.0\n",
    "    \n",
    "    # CIFAR - unknown transition matrix\n",
    "    dataset3 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/CIFAR10.npz')\n",
    "    Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "    Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "    Xts3 = Xts3.astype('float32') / 255.0\n",
    "    \n",
    "    # Define transition matrices\n",
    "    T_fashion_03 = np.array([\n",
    "        [0.7, 0.3, 0, 0],\n",
    "        [0, 0.7, 0.3, 0],\n",
    "        [0, 0, 0.7, 0.3],\n",
    "        [0.3, 0, 0, 0.7]\n",
    "    ])\n",
    "    \n",
    "    T_fashion_06 = np.array([\n",
    "        [0.4, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.4, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.4, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.4]\n",
    "    ])\n",
    "    \n",
    "    # Create transition matrix estimator for CIFAR\n",
    "    print(\"Estimating transition matrix for CIFAR dataset...\")\n",
    "    T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3)\n",
    "    print(\"Estimated transition matrix for CIFAR:\")\n",
    "    print(T_cifar_estimated)\n",
    "    \n",
    "    # Organize datasets for evaluation\n",
    "    # To save time during testing, you might want to only use a small subset of data\n",
    "    # Comment out the datasets you don't want to evaluate\n",
    "    datasets = {\n",
    "        'FashionMNIST_0.3': (Xtr1, Str1, Xts1, Yts1),\n",
    "        'FashionMNIST_0.6': (Xtr2, Str2, Xts2, Yts2),\n",
    "        'CIFAR': (Xtr3, Str3, Xts3, Yts3)\n",
    "    }\n",
    "    \n",
    "    # Define model builders\n",
    "    def build_fashion_mnist_cnn():\n",
    "        model = models.Sequential([\n",
    "            layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4)  # 4 classes\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def build_cifar_cnn():\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(32, 32, 3)),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4)  # 4 classes\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Create classifiers\n",
    "    classifiers = [\n",
    "        # Standard CNN without correction\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Standard CNN\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            use_correction=False\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.3)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.3)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_03,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.6)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.6)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_06,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with estimated transition matrix (for CIFAR)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_estimated)\",\n",
    "            model_builder=build_cifar_cnn,\n",
    "            transition_matrix=T_cifar_estimated,\n",
    "            use_correction=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = run_evaluations(datasets, classifiers, n_runs=3)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Generate results table for report\n",
    "    table = generate_results_table(results)\n",
    "    print(\"\\nResults Table for Report:\")\n",
    "    print(table)\n",
    "    \n",
    "    # Save results to a file\n",
    "    with open(\"results_table.md\", \"w\") as f:\n",
    "        f.write(table)\n",
    "    \n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set CUDA device\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class NoiseRobustClassifier:\n",
    "    def __init__(self, name, model_builder, transition_matrix=None, use_correction=True):\n",
    "        \"\"\"\n",
    "        A wrapper for noise-robust classifiers.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the classifier for reporting\n",
    "            model_builder: Function that returns a PyTorch model\n",
    "            transition_matrix: The noise transition matrix if known\n",
    "            use_correction: Whether to use transition matrix correction\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model_builder = model_builder\n",
    "        self.transition_matrix = torch.tensor(transition_matrix).float().to(device) if transition_matrix is not None else None\n",
    "        self.use_correction = use_correction\n",
    "        self.model = None\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    def forward_correction_loss(self, y_pred, y_true):\n",
    "        \"\"\"Forward correction loss using transition matrix\"\"\"\n",
    "        # Get predicted probabilities\n",
    "        pred_probs = torch.softmax(y_pred, dim=1)\n",
    "        \n",
    "        # Apply transition matrix correction\n",
    "        corrected_probs = torch.matmul(pred_probs, self.transition_matrix)\n",
    "        \n",
    "        # Calculate cross-entropy loss with corrected probabilities\n",
    "        loss = nn.CrossEntropyLoss()(torch.log(corrected_probs + 1e-8), y_true)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X_train, S_train, X_val, S_val, epochs=30, batch_size=128):\n",
    "        \"\"\"Train the classifier on noisy data and validate.\"\"\"\n",
    "        # Convert data to PyTorch tensors and move to GPU\n",
    "        X_train = torch.tensor(X_train).float().to(device)\n",
    "        S_train = torch.tensor(S_train).long().to(device)\n",
    "        X_val = torch.tensor(X_val).float().to(device)\n",
    "        S_val = torch.tensor(S_val).long().to(device)\n",
    "        \n",
    "        # Build model and move to GPU\n",
    "        self.model = self.model_builder().to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = self.forward_correction_loss if (self.transition_matrix is not None and self.use_correction) else nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train[i:i+batch_size]\n",
    "                batch_S = S_train[i:i+batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_S)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_S.size(0)\n",
    "                correct += predicted.eq(batch_S).sum().item()\n",
    "            \n",
    "            train_acc = correct / total\n",
    "            train_loss = train_loss * batch_size / len(X_train)\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    batch_X = X_val[i:i+batch_size]\n",
    "                    batch_S = S_val[i:i+batch_size]\n",
    "                    \n",
    "                    outputs = self.model(batch_X)\n",
    "                    loss = criterion(outputs, batch_S)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += batch_S.size(0)\n",
    "                    correct += predicted.eq(batch_S).sum().item()\n",
    "            \n",
    "            val_acc = correct / total\n",
    "            val_loss = val_loss * batch_size / len(X_val)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 5:\n",
    "                    self.model.load_state_dict(best_state)\n",
    "                    break\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"Evaluate the classifier on clean test data.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Convert data to PyTorch tensors and move to GPU\n",
    "        X_test = torch.tensor(X_test).float().to(device)\n",
    "        Y_test = torch.tensor(Y_test).long().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            _, predicted = outputs.max(1)\n",
    "            accuracy = predicted.eq(Y_test).float().mean().item()\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "def evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs=3):\n",
    "    \"\"\"Evaluate a classifier multiple times with different random splits.\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        seed = 42 + i\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        X_train, X_val, S_train, S_val = train_test_split(\n",
    "            Xtr, Str, test_size=0.2, random_state=seed\n",
    "        )\n",
    "        \n",
    "        print(f\"Run {i+1}/{n_runs} - Training {classifier.name}...\")\n",
    "        \n",
    "        classifier.fit(X_train, S_train, X_val, S_val)\n",
    "        accuracy = classifier.evaluate(Xts, Yts)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Run {i+1}/{n_runs} - Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    return mean_acc, std_acc\n",
    "\n",
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CIFARCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        x = self.dropout1(self.pool(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout2(self.pool(self.relu(self.bn3(self.conv3(x)))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout3(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def build_fashion_mnist_cnn():\n",
    "    return FashionMNISTCNN()\n",
    "\n",
    "def build_cifar_cnn():\n",
    "    return CIFARCNN()\n",
    "\n",
    "def run_evaluations(datasets, classifiers, n_runs=3):\n",
    "    \"\"\"\n",
    "    Run evaluations for multiple classifiers on multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dict of {dataset_name: (Xtr, Str, Xts, Yts)}\n",
    "        classifiers: List of NoiseRobustClassifier instances\n",
    "        n_runs: Number of runs with different random splits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results {dataset_name: {classifier_name: (mean_acc, std_acc)}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, (Xtr, Str, Xts, Yts) in datasets.items():\n",
    "        print(f\"\\n=== Evaluating on dataset: {dataset_name} ===\")\n",
    "        dataset_results = {}\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            print(f\"\\n--- Evaluating classifier: {classifier.name} ---\")\n",
    "            mean_acc, std_acc = evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs)\n",
    "            dataset_results[classifier.name] = (mean_acc, std_acc)\n",
    "            print(f\"Final results - Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        \n",
    "        results[dataset_name] = dataset_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results, title=\"Classifier Performance Comparison\"):\n",
    "    \"\"\"\n",
    "    Create bar plots to visualize classifier performance across datasets.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Determine dimensions\n",
    "    n_datasets = len(results)\n",
    "    n_classifiers = len(next(iter(results.values())))\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 5 * n_datasets))\n",
    "    \n",
    "    # For each dataset\n",
    "    for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "        # Create subplot\n",
    "        plt.subplot(n_datasets, 1, i+1)\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        classifiers = list(dataset_results.keys())\n",
    "        accuracies = [dataset_results[clf][0] for clf in classifiers]  # mean\n",
    "        errors = [dataset_results[clf][1] for clf in classifiers]      # std\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(classifiers, accuracies, yerr=errors, capsize=10)\n",
    "        \n",
    "        # Add labels and grid\n",
    "        plt.title(f\"{dataset_name} - Test Accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                     bar.get_height() + 0.01, \n",
    "                     f\"{acc:.4f}\", \n",
    "                     ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.savefig(\"classifier_performance.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def estimate_transition_matrix_anchor_points(X, S, num_classes=4, anchor_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Estimate transition matrix using anchor points method.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        S: Noisy labels\n",
    "        num_classes: Number of classes\n",
    "        anchor_threshold: Confidence threshold for anchor points\n",
    "        \n",
    "    Returns:\n",
    "        Estimated transition matrix\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data for initial model training\n",
    "    X_train, X_val, S_train, S_val = train_test_split(X, S, test_size=0.2)\n",
    "    \n",
    "    # Create and train a base model\n",
    "    base_model = CIFARCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(base_model.parameters())\n",
    "    \n",
    "    # Convert data to tensors\n",
    "    X_train = torch.tensor(X_train).float().to(device)\n",
    "    S_train = torch.tensor(S_train).long().to(device)\n",
    "    X_val = torch.tensor(X_val).float().to(device)\n",
    "    S_val = torch.tensor(S_val).long().to(device)\n",
    "    X = torch.tensor(X).float().to(device)\n",
    "    \n",
    "    # Train base model\n",
    "    for epoch in range(10):\n",
    "        base_model.train()\n",
    "        for i in range(0, len(X_train), 128):\n",
    "            batch_X = X_train[i:i+128]\n",
    "            batch_S = S_train[i:i+128]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = base_model(batch_X)\n",
    "            loss = criterion(outputs, batch_S)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_probs = []\n",
    "        for i in range(0, len(X), 128):\n",
    "            batch_X = X[i:i+128]\n",
    "            outputs = base_model(batch_X)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_probs.append(probs)\n",
    "        pred_probs = torch.cat(pred_probs, dim=0).cpu().numpy()\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # For each true class, find anchor points and estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        # Find anchor points: points where the model is very confident about class i\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            # Count occurrences of each noisy label among anchor points\n",
    "            for j in range(num_classes):\n",
    "                class_j_count = np.sum(S[anchor_idx] == j)\n",
    "                T[i, j] = class_j_count / len(anchor_idx)\n",
    "        else:\n",
    "            # If no anchor points found, use a fallback (identity or prior knowledge)\n",
    "            T[i, i] = 0.7  # Assuming diagonal dominance as a prior\n",
    "            T[i, (i+1) % num_classes] = 0.3  # Some off-diagonal probability\n",
    "    \n",
    "    # Ensure each row sums to 1\n",
    "    row_sums = T.sum(axis=1, keepdims=True)\n",
    "    T = T / row_sums\n",
    "    \n",
    "    return T\n",
    "\n",
    "def generate_results_table(results):\n",
    "    \"\"\"\n",
    "    Generate a formatted results table for the report.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with results table\n",
    "    \"\"\"\n",
    "    # Create header\n",
    "    table = \"| Dataset | Classifier | Accuracy (%) | Std. Dev. |\\n\"\n",
    "    table += \"|---------|------------|-------------|----------|\\n\"\n",
    "    \n",
    "    # Add rows\n",
    "    for dataset_name, dataset_results in results.items():\n",
    "        for i, (classifier_name, (mean_acc, std_acc)) in enumerate(dataset_results.items()):\n",
    "            # First row of a dataset gets the dataset name\n",
    "            if i == 0:\n",
    "                table += f\"| {dataset_name} | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "            else:\n",
    "                table += f\"| | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "    \n",
    "    return table\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the evaluation framework on GPU.\n",
    "    \"\"\"\n",
    "    print(\"Starting evaluation framework on CUDA:5\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # FashionMNIST 0.3 noise\n",
    "    dataset1 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.3.npz')\n",
    "    Xtr1, Str1 = dataset1['X_tr'], dataset1['S_tr']\n",
    "    Xts1, Yts1 = dataset1['X_ts'], dataset1['Y_ts']\n",
    "    \n",
    "    # Preprocess data (normalize pixel values)\n",
    "    Xtr1 = Xtr1.astype('float32') / 255.0\n",
    "    Xts1 = Xts1.astype('float32') / 255.0\n",
    "    \n",
    "    # FashionMNIST 0.6 noise\n",
    "    dataset2 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.6.npz')\n",
    "    Xtr2, Str2 = dataset2['X_tr'], dataset2['S_tr']\n",
    "    Xts2, Yts2 = dataset2['X_ts'], dataset2['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr2 = Xtr2.astype('float32') / 255.0\n",
    "    Xts2 = Xts2.astype('float32') / 255.0\n",
    "    \n",
    "    # CIFAR - unknown transition matrix\n",
    "    dataset3 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/CIFAR10.npz')\n",
    "    Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "    Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "    Xts3 = Xts3.astype('float32') / 255.0\n",
    "    \n",
    "    # Define transition matrices\n",
    "    T_fashion_03 = np.array([\n",
    "        [0.7, 0.3, 0, 0],\n",
    "        [0, 0.7, 0.3, 0],\n",
    "        [0, 0, 0.7, 0.3],\n",
    "        [0.3, 0, 0, 0.7]\n",
    "    ])\n",
    "    \n",
    "    T_fashion_06 = np.array([\n",
    "        [0.4, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.4, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.4, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.4]\n",
    "    ])\n",
    "    \n",
    "    # Create transition matrix estimator for CIFAR\n",
    "    print(\"Estimating transition matrix for CIFAR dataset...\")\n",
    "    T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3)\n",
    "    print(\"Estimated transition matrix for CIFAR:\")\n",
    "    print(T_cifar_estimated)\n",
    "    \n",
    "    # Organize datasets for evaluation\n",
    "    # To save time during testing, you might want to only use a small subset of data\n",
    "    # Comment out the datasets you don't want to evaluate\n",
    "    datasets = {\n",
    "        'FashionMNIST_0.3': (Xtr1, Str1, Xts1, Yts1),\n",
    "        'FashionMNIST_0.6': (Xtr2, Str2, Xts2, Yts2),\n",
    "        'CIFAR': (Xtr3, Str3, Xts3, Yts3)\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create classifiers\n",
    "    classifiers = [\n",
    "        # Standard CNN without correction\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Standard CNN\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            use_correction=False\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.3)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.3)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_03,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.6)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.6)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_06,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with estimated transition matrix (for CIFAR)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_estimated)\",\n",
    "            model_builder=build_cifar_cnn,\n",
    "            transition_matrix=T_cifar_estimated,\n",
    "            use_correction=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = run_evaluations(datasets, classifiers, n_runs=3)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Generate results table for report\n",
    "    table = generate_results_table(results)\n",
    "    print(\"\\nResults Table for Report:\")\n",
    "    print(table)\n",
    "    \n",
    "    # Save results to a file\n",
    "    with open(\"results_table.md\", \"w\") as f:\n",
    "        f.write(table)\n",
    "    \n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:6\n",
      "Starting evaluation framework on CUDA:5\n",
      "======================================\n",
      "Loading datasets...\n",
      "Estimating transition matrix for CIFAR dataset...\n",
      "Estimated transition matrix for CIFAR:\n",
      "[[0.7        0.3        0.         0.        ]\n",
      " [0.         0.89261745 0.10738255 0.        ]\n",
      " [0.         0.         0.7        0.3       ]\n",
      " [0.3        0.         0.         0.7       ]]\n",
      "\n",
      "=== Evaluating on dataset: FashionMNIST_0.3 ===\n",
      "\n",
      "--- Evaluating classifier: Standard CNN ---\n",
      "Run 1/3 - Training Standard CNN...\n",
      "Epoch 1/30: train_loss=0.9000, val_loss=0.7531, train_acc=0.6093, val_acc=0.6540\n",
      "Epoch 2/30: train_loss=0.7672, val_loss=0.7338, train_acc=0.6535, val_acc=0.6594\n",
      "Epoch 3/30: train_loss=0.7492, val_loss=0.7303, train_acc=0.6614, val_acc=0.6619\n",
      "Epoch 4/30: train_loss=0.7406, val_loss=0.7357, train_acc=0.6632, val_acc=0.6619\n",
      "Epoch 5/30: train_loss=0.7302, val_loss=0.7279, train_acc=0.6659, val_acc=0.6629\n",
      "Epoch 6/30: train_loss=0.7250, val_loss=0.7161, train_acc=0.6643, val_acc=0.6652\n",
      "Epoch 7/30: train_loss=0.7150, val_loss=0.7199, train_acc=0.6701, val_acc=0.6673\n",
      "Epoch 8/30: train_loss=0.7086, val_loss=0.7220, train_acc=0.6740, val_acc=0.6671\n",
      "Epoch 9/30: train_loss=0.6984, val_loss=0.7193, train_acc=0.6747, val_acc=0.6669\n",
      "Epoch 10/30: train_loss=0.6976, val_loss=0.7303, train_acc=0.6779, val_acc=0.6654\n",
      "Run 1/3 - Test accuracy: 0.9538\n",
      "Run 2/3 - Training Standard CNN...\n",
      "Epoch 1/30: train_loss=0.8794, val_loss=0.7619, train_acc=0.6101, val_acc=0.6592\n",
      "Epoch 2/30: train_loss=0.7621, val_loss=0.7381, train_acc=0.6536, val_acc=0.6683\n",
      "Epoch 3/30: train_loss=0.7460, val_loss=0.7352, train_acc=0.6618, val_acc=0.6702\n",
      "Epoch 4/30: train_loss=0.7307, val_loss=0.7368, train_acc=0.6661, val_acc=0.6692\n",
      "Epoch 5/30: train_loss=0.7245, val_loss=0.7287, train_acc=0.6661, val_acc=0.6706\n",
      "Epoch 6/30: train_loss=0.7194, val_loss=0.7358, train_acc=0.6664, val_acc=0.6652\n",
      "Epoch 7/30: train_loss=0.7071, val_loss=0.7308, train_acc=0.6712, val_acc=0.6698\n",
      "Epoch 8/30: train_loss=0.7023, val_loss=0.7186, train_acc=0.6748, val_acc=0.6742\n",
      "Epoch 9/30: train_loss=0.6975, val_loss=0.7166, train_acc=0.6741, val_acc=0.6727\n",
      "Epoch 10/30: train_loss=0.6905, val_loss=0.7277, train_acc=0.6760, val_acc=0.6758\n",
      "Epoch 11/30: train_loss=0.6850, val_loss=0.7232, train_acc=0.6770, val_acc=0.6740\n",
      "Epoch 12/30: train_loss=0.6758, val_loss=0.7246, train_acc=0.6799, val_acc=0.6744\n",
      "Epoch 13/30: train_loss=0.6724, val_loss=0.7349, train_acc=0.6819, val_acc=0.6765\n",
      "Run 2/3 - Test accuracy: 0.9455\n",
      "Run 3/3 - Training Standard CNN...\n",
      "Epoch 1/30: train_loss=0.8446, val_loss=0.7501, train_acc=0.6198, val_acc=0.6485\n",
      "Epoch 2/30: train_loss=0.7500, val_loss=0.7324, train_acc=0.6606, val_acc=0.6587\n",
      "Epoch 3/30: train_loss=0.7315, val_loss=0.7295, train_acc=0.6687, val_acc=0.6538\n",
      "Epoch 4/30: train_loss=0.7251, val_loss=0.7242, train_acc=0.6704, val_acc=0.6550\n",
      "Epoch 5/30: train_loss=0.7091, val_loss=0.7259, train_acc=0.6741, val_acc=0.6585\n",
      "Epoch 6/30: train_loss=0.7069, val_loss=0.7248, train_acc=0.6758, val_acc=0.6658\n",
      "Epoch 7/30: train_loss=0.6995, val_loss=0.7305, train_acc=0.6789, val_acc=0.6585\n",
      "Epoch 8/30: train_loss=0.6916, val_loss=0.7284, train_acc=0.6805, val_acc=0.6562\n",
      "Epoch 9/30: train_loss=0.6864, val_loss=0.7214, train_acc=0.6808, val_acc=0.6663\n",
      "Epoch 10/30: train_loss=0.6813, val_loss=0.7230, train_acc=0.6834, val_acc=0.6633\n",
      "Epoch 11/30: train_loss=0.6786, val_loss=0.7183, train_acc=0.6842, val_acc=0.6629\n",
      "Epoch 12/30: train_loss=0.6688, val_loss=0.7414, train_acc=0.6870, val_acc=0.6613\n",
      "Epoch 13/30: train_loss=0.6628, val_loss=0.7327, train_acc=0.6879, val_acc=0.6617\n",
      "Epoch 14/30: train_loss=0.6603, val_loss=0.7487, train_acc=0.6892, val_acc=0.6583\n",
      "Epoch 15/30: train_loss=0.6549, val_loss=0.7542, train_acc=0.6908, val_acc=0.6637\n",
      "Run 3/3 - Test accuracy: 0.9200\n",
      "Final results - Accuracy: 0.9398 ± 0.0144\n",
      "\n",
      "--- Evaluating classifier: Forward Correction (T_0.3) ---\n",
      "Run 1/3 - Training Forward Correction (T_0.3)...\n",
      "Epoch 1/30: train_loss=0.7910, val_loss=0.7321, train_acc=0.6418, val_acc=0.6579\n",
      "Epoch 2/30: train_loss=0.7252, val_loss=0.7251, train_acc=0.6669, val_acc=0.6617\n",
      "Epoch 3/30: train_loss=0.7088, val_loss=0.7241, train_acc=0.6733, val_acc=0.6613\n",
      "Epoch 4/30: train_loss=0.7003, val_loss=0.7218, train_acc=0.6752, val_acc=0.6617\n",
      "Epoch 5/30: train_loss=0.6935, val_loss=0.7129, train_acc=0.6776, val_acc=0.6710\n",
      "Epoch 6/30: train_loss=0.6842, val_loss=0.7079, train_acc=0.6816, val_acc=0.6679\n",
      "Epoch 7/30: train_loss=0.6789, val_loss=0.7130, train_acc=0.6807, val_acc=0.6683\n",
      "Epoch 8/30: train_loss=0.6742, val_loss=0.7097, train_acc=0.6847, val_acc=0.6685\n",
      "Epoch 9/30: train_loss=0.6695, val_loss=0.7094, train_acc=0.6847, val_acc=0.6715\n",
      "Epoch 10/30: train_loss=0.6636, val_loss=0.7279, train_acc=0.6866, val_acc=0.6679\n",
      "Run 1/3 - Test accuracy: 0.9620\n",
      "Run 2/3 - Training Forward Correction (T_0.3)...\n",
      "Epoch 1/30: train_loss=0.9271, val_loss=0.7448, train_acc=0.5596, val_acc=0.6615\n",
      "Epoch 2/30: train_loss=0.7336, val_loss=0.7210, train_acc=0.6629, val_acc=0.6692\n",
      "Epoch 3/30: train_loss=0.7168, val_loss=0.7119, train_acc=0.6696, val_acc=0.6719\n",
      "Epoch 4/30: train_loss=0.7048, val_loss=0.7065, train_acc=0.6725, val_acc=0.6750\n",
      "Epoch 5/30: train_loss=0.6988, val_loss=0.7032, train_acc=0.6739, val_acc=0.6760\n",
      "Epoch 6/30: train_loss=0.6895, val_loss=0.7080, train_acc=0.6786, val_acc=0.6752\n",
      "Epoch 7/30: train_loss=0.6866, val_loss=0.6989, train_acc=0.6794, val_acc=0.6790\n",
      "Epoch 8/30: train_loss=0.6782, val_loss=0.7026, train_acc=0.6810, val_acc=0.6775\n",
      "Epoch 9/30: train_loss=0.6732, val_loss=0.7038, train_acc=0.6812, val_acc=0.6792\n",
      "Epoch 10/30: train_loss=0.6692, val_loss=0.7048, train_acc=0.6841, val_acc=0.6787\n",
      "Epoch 11/30: train_loss=0.6629, val_loss=0.7009, train_acc=0.6854, val_acc=0.6800\n",
      "Run 2/3 - Test accuracy: 0.9565\n",
      "Run 3/3 - Training Forward Correction (T_0.3)...\n",
      "Epoch 1/30: train_loss=0.7951, val_loss=0.7272, train_acc=0.6380, val_acc=0.6644\n",
      "Epoch 2/30: train_loss=0.7217, val_loss=0.7168, train_acc=0.6683, val_acc=0.6681\n",
      "Epoch 3/30: train_loss=0.7082, val_loss=0.7112, train_acc=0.6750, val_acc=0.6696\n",
      "Epoch 4/30: train_loss=0.6994, val_loss=0.7044, train_acc=0.6766, val_acc=0.6719\n",
      "Epoch 5/30: train_loss=0.6913, val_loss=0.7191, train_acc=0.6790, val_acc=0.6654\n",
      "Epoch 6/30: train_loss=0.6854, val_loss=0.7009, train_acc=0.6812, val_acc=0.6704\n",
      "Epoch 7/30: train_loss=0.6799, val_loss=0.6975, train_acc=0.6818, val_acc=0.6717\n",
      "Epoch 8/30: train_loss=0.6741, val_loss=0.7002, train_acc=0.6858, val_acc=0.6727\n",
      "Epoch 9/30: train_loss=0.6685, val_loss=0.6962, train_acc=0.6870, val_acc=0.6744\n",
      "Epoch 10/30: train_loss=0.6626, val_loss=0.6972, train_acc=0.6884, val_acc=0.6742\n",
      "Epoch 11/30: train_loss=0.6591, val_loss=0.6981, train_acc=0.6889, val_acc=0.6725\n",
      "Epoch 12/30: train_loss=0.6546, val_loss=0.6998, train_acc=0.6897, val_acc=0.6708\n",
      "Epoch 13/30: train_loss=0.6517, val_loss=0.7028, train_acc=0.6920, val_acc=0.6733\n",
      "Run 3/3 - Test accuracy: 0.9580\n",
      "Final results - Accuracy: 0.9588 ± 0.0023\n",
      "\n",
      "--- Evaluating classifier: Forward Correction (T_0.6) ---\n",
      "Run 1/3 - Training Forward Correction (T_0.6)...\n",
      "Epoch 1/30: train_loss=1.1687, val_loss=1.1707, train_acc=0.6345, val_acc=0.6531\n",
      "Epoch 2/30: train_loss=1.1488, val_loss=1.1665, train_acc=0.6636, val_acc=0.6598\n",
      "Epoch 3/30: train_loss=1.1448, val_loss=1.1632, train_acc=0.6695, val_acc=0.6663\n",
      "Epoch 4/30: train_loss=1.1426, val_loss=1.1640, train_acc=0.6723, val_acc=0.6640\n",
      "Epoch 5/30: train_loss=1.1421, val_loss=1.1618, train_acc=0.6732, val_acc=0.6677\n",
      "Epoch 6/30: train_loss=1.1425, val_loss=1.1635, train_acc=0.6724, val_acc=0.6646\n",
      "Epoch 7/30: train_loss=1.1406, val_loss=1.1639, train_acc=0.6756, val_acc=0.6646\n",
      "Epoch 8/30: train_loss=1.1391, val_loss=1.1649, train_acc=0.6772, val_acc=0.6635\n",
      "Epoch 9/30: train_loss=1.1378, val_loss=1.1612, train_acc=0.6794, val_acc=0.6681\n",
      "Epoch 10/30: train_loss=1.1370, val_loss=1.1610, train_acc=0.6810, val_acc=0.6696\n",
      "Epoch 11/30: train_loss=1.1357, val_loss=1.1624, train_acc=0.6828, val_acc=0.6654\n",
      "Epoch 12/30: train_loss=1.1356, val_loss=1.1630, train_acc=0.6830, val_acc=0.6652\n",
      "Epoch 13/30: train_loss=1.1350, val_loss=1.1606, train_acc=0.6841, val_acc=0.6690\n",
      "Epoch 14/30: train_loss=1.1335, val_loss=1.1617, train_acc=0.6856, val_acc=0.6671\n",
      "Epoch 15/30: train_loss=1.1334, val_loss=1.1619, train_acc=0.6857, val_acc=0.6667\n",
      "Epoch 16/30: train_loss=1.1323, val_loss=1.1632, train_acc=0.6881, val_acc=0.6654\n",
      "Epoch 17/30: train_loss=1.1315, val_loss=1.1613, train_acc=0.6885, val_acc=0.6675\n",
      "Run 1/3 - Test accuracy: 0.9450\n",
      "Run 2/3 - Training Forward Correction (T_0.6)...\n",
      "Epoch 1/30: train_loss=1.1778, val_loss=1.1698, train_acc=0.6212, val_acc=0.6562\n",
      "Epoch 2/30: train_loss=1.1500, val_loss=1.1723, train_acc=0.6622, val_acc=0.6527\n",
      "Epoch 3/30: train_loss=1.1463, val_loss=1.1645, train_acc=0.6677, val_acc=0.6633\n",
      "Epoch 4/30: train_loss=1.1443, val_loss=1.1602, train_acc=0.6699, val_acc=0.6706\n",
      "Epoch 5/30: train_loss=1.1424, val_loss=1.1634, train_acc=0.6727, val_acc=0.6648\n",
      "Epoch 6/30: train_loss=1.1422, val_loss=1.1641, train_acc=0.6723, val_acc=0.6656\n",
      "Epoch 7/30: train_loss=1.1417, val_loss=1.1606, train_acc=0.6737, val_acc=0.6704\n",
      "Epoch 8/30: train_loss=1.1399, val_loss=1.1620, train_acc=0.6766, val_acc=0.6687\n",
      "Run 2/3 - Test accuracy: 0.9438\n",
      "Run 3/3 - Training Forward Correction (T_0.6)...\n",
      "Epoch 1/30: train_loss=1.1634, val_loss=1.1791, train_acc=0.6436, val_acc=0.6417\n",
      "Epoch 2/30: train_loss=1.1516, val_loss=1.1681, train_acc=0.6591, val_acc=0.6585\n",
      "Epoch 3/30: train_loss=1.1458, val_loss=1.1673, train_acc=0.6684, val_acc=0.6579\n",
      "Epoch 4/30: train_loss=1.1435, val_loss=1.1621, train_acc=0.6706, val_acc=0.6679\n",
      "Epoch 5/30: train_loss=1.1423, val_loss=1.1602, train_acc=0.6731, val_acc=0.6700\n",
      "Epoch 6/30: train_loss=1.1409, val_loss=1.1618, train_acc=0.6748, val_acc=0.6665\n",
      "Epoch 7/30: train_loss=1.1392, val_loss=1.1616, train_acc=0.6771, val_acc=0.6675\n",
      "Epoch 8/30: train_loss=1.1391, val_loss=1.1674, train_acc=0.6780, val_acc=0.6581\n",
      "Epoch 9/30: train_loss=1.1371, val_loss=1.1620, train_acc=0.6808, val_acc=0.6675\n",
      "Run 3/3 - Test accuracy: 0.9448\n",
      "Final results - Accuracy: 0.9445 ± 0.0005\n",
      "\n",
      "--- Evaluating classifier: Forward Correction (T_estimated) ---\n",
      "Run 1/3 - Training Forward Correction (T_estimated)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3, 32, 32]' is invalid for input of size 100352",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 532\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 515\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Standard CNN without correction\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     NoiseRobustClassifier(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     )\n\u001b[1;32m    512\u001b[0m ]\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# Run evaluations\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    518\u001b[0m visualize_results(results)\n",
      "Cell \u001b[0;32mIn[4], line 251\u001b[0m, in \u001b[0;36mrun_evaluations\u001b[0;34m(datasets, classifiers, n_runs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Evaluating classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     mean_acc, std_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     dataset_results[classifier\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m (mean_acc, std_acc)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal results - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 164\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[0;34m(classifier, Xtr, Str, Xts, Yts, n_runs)\u001b[0m\n\u001b[1;32m    158\u001b[0m X_train, X_val, S_train, S_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    159\u001b[0m     Xtr, Str, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mevaluate(Xts, Yts)\n\u001b[1;32m    166\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m, in \u001b[0;36mNoiseRobustClassifier.fit\u001b[0;34m(self, X_train, S_train, X_val, S_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m batch_S \u001b[38;5;241m=\u001b[39m S_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_S)\n\u001b[1;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml817/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml817/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 216\u001b[0m, in \u001b[0;36mCIFARCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 216\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))))\n\u001b[1;32m    218\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3, 32, 32]' is invalid for input of size 100352"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set CUDA device\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class NoiseRobustClassifier:\n",
    "    def __init__(self, name, model_builder, transition_matrix=None, use_correction=True):\n",
    "        \"\"\"\n",
    "        A wrapper for noise-robust classifiers.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the classifier for reporting\n",
    "            model_builder: Function that returns a PyTorch model\n",
    "            transition_matrix: The noise transition matrix if known\n",
    "            use_correction: Whether to use transition matrix correction\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model_builder = model_builder\n",
    "        self.transition_matrix = torch.tensor(transition_matrix).float().to(device) if transition_matrix is not None else None\n",
    "        self.use_correction = use_correction\n",
    "        self.model = None\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    def forward_correction_loss(self, y_pred, y_true):\n",
    "        \"\"\"Forward correction loss using transition matrix\"\"\"\n",
    "        # Get predicted probabilities\n",
    "        pred_probs = torch.softmax(y_pred, dim=1)\n",
    "        \n",
    "        # Apply transition matrix correction\n",
    "        corrected_probs = torch.matmul(pred_probs, self.transition_matrix)\n",
    "        \n",
    "        # Calculate cross-entropy loss with corrected probabilities\n",
    "        loss = nn.CrossEntropyLoss()(torch.log(corrected_probs + 1e-8), y_true)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X_train, S_train, X_val, S_val, epochs=30, batch_size=128):\n",
    "        \"\"\"Train the classifier on noisy data and validate.\"\"\"\n",
    "        # Convert data to PyTorch tensors and move to GPU\n",
    "        X_train = torch.tensor(X_train).float().to(device)\n",
    "        S_train = torch.tensor(S_train).long().to(device)\n",
    "        X_val = torch.tensor(X_val).float().to(device)\n",
    "        S_val = torch.tensor(S_val).long().to(device)\n",
    "        \n",
    "        # Build model and move to GPU\n",
    "        self.model = self.model_builder().to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = self.forward_correction_loss if (self.transition_matrix is not None and self.use_correction) else nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train[i:i+batch_size]\n",
    "                batch_S = S_train[i:i+batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_S)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_S.size(0)\n",
    "                correct += predicted.eq(batch_S).sum().item()\n",
    "            \n",
    "            train_acc = correct / total\n",
    "            train_loss = train_loss * batch_size / len(X_train)\n",
    "            \n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    batch_X = X_val[i:i+batch_size]\n",
    "                    batch_S = S_val[i:i+batch_size]\n",
    "                    \n",
    "                    outputs = self.model(batch_X)\n",
    "                    loss = criterion(outputs, batch_S)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += batch_S.size(0)\n",
    "                    correct += predicted.eq(batch_S).sum().item()\n",
    "            \n",
    "            val_acc = correct / total\n",
    "            val_loss = val_loss * batch_size / len(X_val)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 5:\n",
    "                    self.model.load_state_dict(best_state)\n",
    "                    break\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"Evaluate the classifier on clean test data.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Convert data to PyTorch tensors and move to GPU\n",
    "        X_test = torch.tensor(X_test).float().to(device)\n",
    "        Y_test = torch.tensor(Y_test).long().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            _, predicted = outputs.max(1)\n",
    "            accuracy = predicted.eq(Y_test).float().mean().item()\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "def evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs=3):\n",
    "    \"\"\"Evaluate a classifier multiple times with different random splits.\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        seed = 42 + i\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        X_train, X_val, S_train, S_val = train_test_split(\n",
    "            Xtr, Str, test_size=0.2, random_state=seed\n",
    "        )\n",
    "        \n",
    "        print(f\"Run {i+1}/{n_runs} - Training {classifier.name}...\")\n",
    "        \n",
    "        classifier.fit(X_train, S_train, X_val, S_val)\n",
    "        accuracy = classifier.evaluate(Xts, Yts)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Run {i+1}/{n_runs} - Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    return mean_acc, std_acc\n",
    "\n",
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CIFARCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 32, 32)\n",
    "        x = self.dropout1(self.pool(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout2(self.pool(self.relu(self.bn3(self.conv3(x)))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout3(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def build_fashion_mnist_cnn():\n",
    "    return FashionMNISTCNN()\n",
    "\n",
    "def build_cifar_cnn():\n",
    "    return CIFARCNN()\n",
    "\n",
    "def run_evaluations(datasets, classifiers, n_runs=3):\n",
    "    \"\"\"\n",
    "    Run evaluations for multiple classifiers on multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dict of {dataset_name: (Xtr, Str, Xts, Yts)}\n",
    "        classifiers: List of NoiseRobustClassifier instances\n",
    "        n_runs: Number of runs with different random splits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results {dataset_name: {classifier_name: (mean_acc, std_acc)}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, (Xtr, Str, Xts, Yts) in datasets.items():\n",
    "        print(f\"\\n=== Evaluating on dataset: {dataset_name} ===\")\n",
    "        dataset_results = {}\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            print(f\"\\n--- Evaluating classifier: {classifier.name} ---\")\n",
    "            mean_acc, std_acc = evaluate_classifier(classifier, Xtr, Str, Xts, Yts, n_runs)\n",
    "            dataset_results[classifier.name] = (mean_acc, std_acc)\n",
    "            print(f\"Final results - Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        \n",
    "        results[dataset_name] = dataset_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results, title=\"Classifier Performance Comparison\"):\n",
    "    \"\"\"\n",
    "    Create bar plots to visualize classifier performance across datasets.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Determine dimensions\n",
    "    n_datasets = len(results)\n",
    "    n_classifiers = len(next(iter(results.values())))\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 5 * n_datasets))\n",
    "    \n",
    "    # For each dataset\n",
    "    for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "        # Create subplot\n",
    "        plt.subplot(n_datasets, 1, i+1)\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        classifiers = list(dataset_results.keys())\n",
    "        accuracies = [dataset_results[clf][0] for clf in classifiers]  # mean\n",
    "        errors = [dataset_results[clf][1] for clf in classifiers]      # std\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(classifiers, accuracies, yerr=errors, capsize=10)\n",
    "        \n",
    "        # Add labels and grid\n",
    "        plt.title(f\"{dataset_name} - Test Accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add accuracy values on top of bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                     bar.get_height() + 0.01, \n",
    "                     f\"{acc:.4f}\", \n",
    "                     ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.savefig(\"classifier_performance.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def estimate_transition_matrix_anchor_points(X, S, num_classes=4, anchor_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Estimate transition matrix using anchor points method.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        S: Noisy labels\n",
    "        num_classes: Number of classes\n",
    "        anchor_threshold: Confidence threshold for anchor points\n",
    "        \n",
    "    Returns:\n",
    "        Estimated transition matrix\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data for initial model training\n",
    "    X_train, X_val, S_train, S_val = train_test_split(X, S, test_size=0.2)\n",
    "    \n",
    "    # Create and train a base model\n",
    "    base_model = CIFARCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(base_model.parameters())\n",
    "    \n",
    "    # Convert data to tensors\n",
    "    X_train = torch.tensor(X_train).float().to(device)\n",
    "    S_train = torch.tensor(S_train).long().to(device)\n",
    "    X_val = torch.tensor(X_val).float().to(device)\n",
    "    S_val = torch.tensor(S_val).long().to(device)\n",
    "    X = torch.tensor(X).float().to(device)\n",
    "    \n",
    "    # Train base model\n",
    "    for epoch in range(10):\n",
    "        base_model.train()\n",
    "        for i in range(0, len(X_train), 128):\n",
    "            batch_X = X_train[i:i+128]\n",
    "            batch_S = S_train[i:i+128]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = base_model(batch_X)\n",
    "            loss = criterion(outputs, batch_S)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_probs = []\n",
    "        for i in range(0, len(X), 128):\n",
    "            batch_X = X[i:i+128]\n",
    "            outputs = base_model(batch_X)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_probs.append(probs)\n",
    "        pred_probs = torch.cat(pred_probs, dim=0).cpu().numpy()\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # For each true class, find anchor points and estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        # Find anchor points: points where the model is very confident about class i\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            # Count occurrences of each noisy label among anchor points\n",
    "            for j in range(num_classes):\n",
    "                class_j_count = np.sum(S[anchor_idx] == j)\n",
    "                T[i, j] = class_j_count / len(anchor_idx)\n",
    "        else:\n",
    "            # If no anchor points found, use a fallback (identity or prior knowledge)\n",
    "            T[i, i] = 0.7  # Assuming diagonal dominance as a prior\n",
    "            T[i, (i+1) % num_classes] = 0.3  # Some off-diagonal probability\n",
    "    \n",
    "    # Ensure each row sums to 1\n",
    "    row_sums = T.sum(axis=1, keepdims=True)\n",
    "    T = T / row_sums\n",
    "    \n",
    "    return T\n",
    "\n",
    "def generate_results_table(results):\n",
    "    \"\"\"\n",
    "    Generate a formatted results table for the report.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from run_evaluations\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with results table\n",
    "    \"\"\"\n",
    "    # Create header\n",
    "    table = \"| Dataset | Classifier | Accuracy (%) | Std. Dev. |\\n\"\n",
    "    table += \"|---------|------------|-------------|----------|\\n\"\n",
    "    \n",
    "    # Add rows\n",
    "    for dataset_name, dataset_results in results.items():\n",
    "        for i, (classifier_name, (mean_acc, std_acc)) in enumerate(dataset_results.items()):\n",
    "            # First row of a dataset gets the dataset name\n",
    "            if i == 0:\n",
    "                table += f\"| {dataset_name} | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "            else:\n",
    "                table += f\"| | {classifier_name} | {mean_acc*100:.2f} | {std_acc*100:.2f} |\\n\"\n",
    "    \n",
    "    return table\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the evaluation framework on GPU.\n",
    "    \"\"\"\n",
    "    print(\"Starting evaluation framework on CUDA:5\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    # FashionMNIST 0.3 noise\n",
    "    dataset1 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.3.npz')\n",
    "    Xtr1, Str1 = dataset1['X_tr'], dataset1['S_tr']\n",
    "    Xts1, Yts1 = dataset1['X_ts'], dataset1['Y_ts']\n",
    "    \n",
    "    # Preprocess data (normalize pixel values)\n",
    "    Xtr1 = Xtr1.astype('float32') / 255.0\n",
    "    Xts1 = Xts1.astype('float32') / 255.0\n",
    "    \n",
    "    # FashionMNIST 0.6 noise\n",
    "    dataset2 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/FashionMNIST0.6.npz')\n",
    "    Xtr2, Str2 = dataset2['X_tr'], dataset2['S_tr']\n",
    "    Xts2, Yts2 = dataset2['X_ts'], dataset2['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr2 = Xtr2.astype('float32') / 255.0\n",
    "    Xts2 = Xts2.astype('float32') / 255.0\n",
    "    \n",
    "    # CIFAR - unknown transition matrix\n",
    "    dataset3 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/CIFAR10.npz')\n",
    "    Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "    Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "    \n",
    "    # Preprocess data\n",
    "    Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "    Xts3 = Xts3.astype('float32') / 255.0\n",
    "    \n",
    "    # Define transition matrices\n",
    "    T_fashion_03 = np.array([\n",
    "        [0.7, 0.3, 0, 0],\n",
    "        [0, 0.7, 0.3, 0],\n",
    "        [0, 0, 0.7, 0.3],\n",
    "        [0.3, 0, 0, 0.7]\n",
    "    ])\n",
    "    \n",
    "    T_fashion_06 = np.array([\n",
    "        [0.4, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.4, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.4, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.4]\n",
    "    ])\n",
    "    \n",
    "    # Create transition matrix estimator for CIFAR\n",
    "    print(\"Estimating transition matrix for CIFAR dataset...\")\n",
    "    T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3)\n",
    "    print(\"Estimated transition matrix for CIFAR:\")\n",
    "    print(T_cifar_estimated)\n",
    "    \n",
    "    # Organize datasets for evaluation\n",
    "    # To save time during testing, you might want to only use a small subset of data\n",
    "    # Comment out the datasets you don't want to evaluate\n",
    "    datasets = {\n",
    "        'FashionMNIST_0.3': (Xtr1, Str1, Xts1, Yts1),\n",
    "        'FashionMNIST_0.6': (Xtr2, Str2, Xts2, Yts2),\n",
    "        'CIFAR': (Xtr3, Str3, Xts3, Yts3)\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create classifiers\n",
    "    classifiers = [\n",
    "        # Standard CNN without correction\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Standard CNN\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            use_correction=False\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.3)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.3)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_03,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with known transition matrix (for FashionMNIST_0.6)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_0.6)\",\n",
    "            model_builder=build_fashion_mnist_cnn,\n",
    "            transition_matrix=T_fashion_06,\n",
    "            use_correction=True\n",
    "        ),\n",
    "        \n",
    "        # Forward Correction with estimated transition matrix (for CIFAR)\n",
    "        NoiseRobustClassifier(\n",
    "            name=\"Forward Correction (T_estimated)\",\n",
    "            model_builder=build_cifar_cnn,\n",
    "            transition_matrix=T_cifar_estimated,\n",
    "            use_correction=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Run evaluations\n",
    "    results = run_evaluations(datasets, classifiers, n_runs=3)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(results)\n",
    "    \n",
    "    # Generate results table for report\n",
    "    table = generate_results_table(results)\n",
    "    print(\"\\nResults Table for Report:\")\n",
    "    print(table)\n",
    "    \n",
    "    # Save results to a file\n",
    "    with open(\"results_table.md\", \"w\") as f:\n",
    "        f.write(table)\n",
    "    \n",
    "    print(\"\\nEvaluation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating transition matrix for CIFAR dataset...\n",
      "Base model test accuracy: 0.8167\n",
      "Estimated transition matrix for CIFAR:\n",
      "[[0.94202899 0.04057971 0.00869565 0.00869565]\n",
      " [0.02216066 0.92520776 0.05124654 0.00138504]\n",
      " [0.00257732 0.         0.95360825 0.04381443]\n",
      " [0.0620438  0.         0.00547445 0.93248175]]\n"
     ]
    }
   ],
   "source": [
    "# Find transition matrix for CIFAR10\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CIFAR\n",
    "dataset3 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/CIFAR10.npz')\n",
    "Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "\n",
    "# Normalize data\n",
    "Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "Xts3 = Xts3.astype('float32') / 255.0\n",
    "\n",
    "# Transpose data to match PyTorch's expected format (N, C, H, W)\n",
    "Xtr3 = np.transpose(Xtr3, (0, 3, 1, 2))\n",
    "Xts3 = np.transpose(Xts3, (0, 3, 1, 2))\n",
    "\n",
    "def estimate_transition_matrix_anchor_points(Xtr, Str, Xts, Yts, num_classes=4, anchor_threshold=0.9):\n",
    "    \"\"\"Estimate transition matrix using anchor points method.\"\"\"\n",
    "    X_train, X_val, S_train, S_val = train_test_split(Xtr, Str, test_size=0.2)\n",
    "    \n",
    "    # Create base model (ResNet-18)\n",
    "    class BasicBlock(nn.Module):\n",
    "        expansion = 1\n",
    "        def __init__(self, in_planes, planes, stride=1):\n",
    "            super(BasicBlock, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if stride != 1 or in_planes != self.expansion*planes:\n",
    "                self.shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(self.expansion*planes)\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.relu(self.bn1(self.conv1(x)))\n",
    "            out = self.bn2(self.conv2(out))\n",
    "            out += self.shortcut(x)\n",
    "            out = torch.relu(out)\n",
    "            return out\n",
    "\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self, block, num_blocks, num_classes=4):\n",
    "            super(ResNet, self).__init__()\n",
    "            self.in_planes = 64\n",
    "\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "            self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "            self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "            self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "            self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "        def _make_layer(self, block, planes, num_blocks, stride):\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            layers = []\n",
    "            for stride in strides:\n",
    "                layers.append(block(self.in_planes, planes, stride))\n",
    "                self.in_planes = planes * block.expansion\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.relu(self.bn1(self.conv1(x)))\n",
    "            out = self.layer1(out)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = self.layer4(out)\n",
    "            out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "            return out\n",
    "\n",
    "    base_model = ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes).to(device)\n",
    "    \n",
    "    # Train base model\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device),\n",
    "                                torch.tensor(S_train, dtype=torch.long).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(base_model.parameters())\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        base_model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = base_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 128\n",
    "        pred_probs = []\n",
    "        for i in range(0, len(Xtr), batch_size):\n",
    "            batch = torch.tensor(Xtr[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            pred_probs.append(torch.softmax(base_model(batch), dim=1).cpu().numpy())\n",
    "        pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "        \n",
    "        # Evaluate accuracy on test set\n",
    "        test_preds = []\n",
    "        for i in range(0, len(Xts), batch_size):\n",
    "            batch = torch.tensor(Xts[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            test_preds.append(torch.argmax(base_model(batch), dim=1).cpu().numpy())\n",
    "        test_preds = np.concatenate(test_preds)\n",
    "        test_accuracy = np.mean(test_preds == Yts)\n",
    "        print(f\"Base model test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # Estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            for j in range(num_classes):\n",
    "                T[i, j] = np.mean(Str[anchor_idx] == j)\n",
    "        else:\n",
    "            T[i, i] = 0.7\n",
    "            T[i, (i+1) % num_classes] = 0.3\n",
    "    \n",
    "    # Normalize rows\n",
    "    T = T / T.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Estimate transition matrix for CIFAR\n",
    "print(\"Estimating transition matrix for CIFAR dataset...\")\n",
    "T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3, Xts3, Yts3, num_classes=4, anchor_threshold=0.9)\n",
    "print(\"Estimated transition matrix for CIFAR:\")\n",
    "print(T_cifar_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model test accuracy: 0.8300\n",
      "Base model test accuracy: 0.2557\n"
     ]
    }
   ],
   "source": [
    "# Classification with anchor points\n",
    "Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "Xts3 = Xts3.astype('float32') / 255.0\n",
    "num_classes = 4\n",
    "\n",
    "# Estimate transition matrix\n",
    "T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3, Xts3, Yts3, num_classes=4, anchor_threshold=0.9)\n",
    "\n",
    "# Create base model (ResNet-18)\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "base_model = ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes).to(device)\n",
    "    \n",
    "# Evaluate accuracy on test set\n",
    "X_test_tensor = torch.tensor(Xts3, dtype=torch.float32).to(device)\n",
    "test_outputs = base_model(X_test_tensor)\n",
    "test_preds = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "test_accuracy = np.mean(test_preds == Yts3)\n",
    "print(f\"Base model test accuracy: {test_accuracy:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating transition matrix for CIFAR dataset...\n",
      "\n",
      "Run 1/3\n"
     ]
    }
   ],
   "source": [
    "# Find transition matrix for CIFAR10\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# CIFAR\n",
    "dataset3 = np.load('/home/aidar.alimbayev/Documents/projects_aidar/ml814_lab/labs/datasets/CIFAR10.npz')\n",
    "Xtr3, Str3 = dataset3['X_tr'], dataset3['S_tr']\n",
    "Xts3, Yts3 = dataset3['X_ts'], dataset3['Y_ts']\n",
    "\n",
    "# Normalize data\n",
    "Xtr3 = Xtr3.astype('float32') / 255.0\n",
    "Xts3 = Xts3.astype('float32') / 255.0\n",
    "\n",
    "# Transpose data to match PyTorch's expected format (N, C, H, W)\n",
    "Xtr3 = np.transpose(Xtr3, (0, 3, 1, 2))\n",
    "Xts3 = np.transpose(Xts3, (0, 3, 1, 2))\n",
    "\n",
    "def estimate_transition_matrix_anchor_points(Xtr, Str, Xts, Yts, num_classes=4, anchor_threshold=0.9, num_runs=3):\n",
    "    \"\"\"Estimate transition matrix using anchor points method.\"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\nRun {run+1}/{num_runs}\")\n",
    "        X_train, X_val, S_train, S_val = train_test_split(Xtr, Str, test_size=0.2)\n",
    "        \n",
    "        # Create base model (ResNet-18)\n",
    "        class BasicBlock(nn.Module):\n",
    "            expansion = 1\n",
    "            def __init__(self, in_planes, planes, stride=1):\n",
    "                super(BasicBlock, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(planes)\n",
    "                self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "                self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "                self.shortcut = nn.Sequential()\n",
    "                if stride != 1 or in_planes != self.expansion*planes:\n",
    "                    self.shortcut = nn.Sequential(\n",
    "                        nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm2d(self.expansion*planes)\n",
    "                    )\n",
    "\n",
    "            def forward(self, x):\n",
    "                out = torch.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.bn2(self.conv2(out))\n",
    "                out += self.shortcut(x)\n",
    "                out = torch.relu(out)\n",
    "                return out\n",
    "\n",
    "        class ResNet(nn.Module):\n",
    "            def __init__(self, block, num_blocks, num_classes=4):\n",
    "                super(ResNet, self).__init__()\n",
    "                self.in_planes = 64\n",
    "\n",
    "                self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(64)\n",
    "                self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "                self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "                self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "                self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "                self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "            def _make_layer(self, block, planes, num_blocks, stride):\n",
    "                strides = [stride] + [1]*(num_blocks-1)\n",
    "                layers = []\n",
    "                for stride in strides:\n",
    "                    layers.append(block(self.in_planes, planes, stride))\n",
    "                    self.in_planes = planes * block.expansion\n",
    "                return nn.Sequential(*layers)\n",
    "\n",
    "            def forward(self, x):\n",
    "                out = torch.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.layer1(out)\n",
    "                out = self.layer2(out)\n",
    "                out = self.layer3(out)\n",
    "                out = self.layer4(out)\n",
    "                out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "                out = out.view(out.size(0), -1)\n",
    "                out = self.linear(out)\n",
    "                return out\n",
    "\n",
    "        # ResNet-18\n",
    "        base_model = ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes).to(device)\n",
    "        \n",
    "        # Train base model\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device),\n",
    "                                    torch.tensor(S_train, dtype=torch.long).to(device))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(base_model.parameters())\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            base_model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = base_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Get predicted probabilities\n",
    "        base_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Process in batches to avoid memory issues\n",
    "            batch_size = 128\n",
    "            pred_probs = []\n",
    "            for i in range(0, len(Xtr), batch_size):\n",
    "                batch = torch.tensor(Xtr[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "                pred_probs.append(torch.softmax(base_model(batch), dim=1).cpu().numpy())\n",
    "            pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "            \n",
    "            # Evaluate accuracy on test set\n",
    "            test_preds = []\n",
    "            for i in range(0, len(Xts), batch_size):\n",
    "                batch = torch.tensor(Xts[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "                test_preds.append(torch.argmax(base_model(batch), dim=1).cpu().numpy())\n",
    "            test_preds = np.concatenate(test_preds)\n",
    "            test_accuracy = np.mean(test_preds == Yts)\n",
    "            print(f\"Base model test accuracy: {test_accuracy:.4f}\")\n",
    "            accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Calculate mean and std of accuracies\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    print(f\"\\nAccuracy across {num_runs} runs: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    \n",
    "    # Initialize transition matrix\n",
    "    T = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    # Estimate transition probabilities\n",
    "    for i in range(num_classes):\n",
    "        anchor_idx = np.where(pred_probs[:, i] >= anchor_threshold)[0]\n",
    "        \n",
    "        if len(anchor_idx) > 0:\n",
    "            for j in range(num_classes):\n",
    "                T[i, j] = np.mean(Str[anchor_idx] == j)\n",
    "        else:\n",
    "            T[i, i] = 0.7\n",
    "            T[i, (i+1) % num_classes] = 0.3\n",
    "    \n",
    "    # Normalize rows\n",
    "    T = T / T.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Estimate transition matrix for CIFAR\n",
    "print(\"Estimating transition matrix for CIFAR dataset...\")\n",
    "T_cifar_estimated = estimate_transition_matrix_anchor_points(Xtr3, Str3, Xts3, Yts3, num_classes=4, anchor_threshold=0.9)\n",
    "print(\"Estimated transition matrix for CIFAR:\")\n",
    "print(T_cifar_estimated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml817",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
